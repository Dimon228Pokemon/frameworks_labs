# 2. Создание бейзлайна и оценка качества

## 2.a. Baseline модели sklearn

В качестве отправной точки используются стандартные модели:

- **Классификация**: `LogisticRegression(max_iter=200)`
- **Регрессия**: `LinearRegression()`

Перед обучением числовые признаки были масштабированы с помощью `StandardScaler`, а категориальные преобразованы через `OneHotEncoder`. Данные разделены на обучающую и тестовую выборки.

## 2.b. Качество baseline моделей

### Классификация - Baseline Logistic Regression

| Метрика    | Значение |
|------------|----------|
| Accuracy   | 0.809    |
| Precision  | 0.6929   |
| Recall     | 0.2449   |
| F1         | 0.3619   |
| ROC-AUC    | 0.7101   |

**Вывод**: Модель уверенно предсказывает недефолтных клиентов, но пропускает значительную часть дефолтов. Низкий recall и F1 указывают на необходимость улучшения.

### Регрессия - Baseline Linear Regression

| Метрика | Значение |
|---------|----------|
| MAE     | 127 731  |
| RMSE    | 164 012  |
| R^2      | 0.9143   |

**Вывод**: Базовое качество высокое, однако есть возможность улучшить модель за счёт более гибких признаков и регуляризации.

# 3. Улучшение бейзлайна

## 3.a. Сформулированные гипотезы

**Классификация (Logistic Regression)**
- использование L1/L2 регуляризации позволит модели обучаться стабильнее;
- применение `class_weight="balanced"` повысит recall для дефолтов;
- подбор параметров через GridSearch должен улучшить F1;
- увеличение `max_iter` может улучшить сходимость.

**Регрессия (Linear Regression)**
- регуляризация Ridge уменьшит переобучение;
- добавление `PolynomialFeatures` степени 2 расширит пространство признаков и повысит точность;
- подбор параметра `alpha` даст рост R^2;
- более тщательно настроенный препроцессинг снизит MAE.

## 3.b. Проверка гипотез

**Logistic Regression - GridSearchCV**

Наилучшая модель получила параметры:
- `penalty`: L1
- `C`: 0.1
- `class_weight`: balanced
- `solver`: liblinear

**Ridge Regression + PolynomialFeatures(2)**

Лучший параметр регуляризации:
- `alpha`: 3

### 3.c. Улучшенный бейзлайн

**Классификация**
```python
LogisticRegression(
    penalty='l1',
    C=0.1,
    class_weight='balanced',
    solver='liblinear'
)
```
**Регрессия**
```python
Pipeline([
    PolynomialFeatures(degree=2),
    Ridge(alpha=3)
])
```
## 3.d - 3.e. Оценка улучшенных моделей

### Классификация - Improved Logistic Regression

| Метрика   | Baseline | Improved |
|-----------|----------|----------|
| F1        | 0.3619   | 0.4666   |
| Recall    | 0.2449   | 0.6323   |
| ROC-AUC   | 0.7101   | 0.7091   |

**Вывод:** Улучшенная модель значительно увеличивает recall и F1 (в 1.3x-1.8x раза), что критично для задач поиска дефолтных клиентов.

### Регрессия - Ridge + PolynomialFeatures(2)

| Метрика | Baseline | Improved |
|---------|----------|----------|
| MAE     | 127 731  | 126 863  |
| RMSE    | 164 012  | 162 351  |
| R^2      | 0.9143   | 0.9160   |

**Вывод:** Достигнут устойчивый прирост качества за счёт регуляризации и полиномиальных признаков.

## 3.f. Сравнение результатов

В классификации наблюдается заметный рост F1 и Recall, что делает модель гораздо эффективнее с практической точки зрения. В регрессии улучшение умеренное, но стабильное: ошибка снижается, а модель лучше объясняет вариацию в данных.

## 3.g. Выводы

Использование регуляризации и расширение признаков оправдало себя в обеих задачах. Logistic Regression оказалась чувствительной к дисбалансу классов, поэтому применение `class_weight` стало ключевым фактором улучшения. Линейная регрессия в улучшенном варианте стала более устойчивой и точной.

# 4. Имплементация алгоритмов вручную

## 4.a. Реализация методов

Были реализованы:

- `MyLinearRegression` - нормальное уравнение;
- `MyRidgeRegression` - нормальное уравнение + L2 регуляризация;
- `MyLogisticRegression` - базовая версия с градиентным спуском;
- `MyLogisticImproved` - улучшенная модель с регуляризацией, class weights и более стабильным обучением.

Все модели поддерживают bias-параметр.

## 4.b - 4.c. Обучение и оценка self-made моделей

#### Линейная регрессия - self-made

| Модель              | MAE      | RMSE     | R^2       |
|---------------------|----------|----------|----------|
| MyLinear + Poly     | 126 844  | 162 410  | 0.91593  |
| MyRidge + Poly      | 126 831  | 162 365  | 0.91598  |

**Вывод:** Ручные реализации дают результат, сопоставимый с sklearn, что подтверждает корректность формул и вычислений.

#### Логистическая регрессия — self-made

| Модель               | F1       | ROC-AUC  |
|----------------------|----------|----------|
| MyLogistic baseline  | 0.3616   | 0.7085   |
| MyLogisticImproved   | 0.4387   | 0.6839   |

**Вывод:** Улучшенная версия значительно повышает F1 и повторяет тенденции sklearn-модели.

## 4.d. Сравнение self-made vs sklearn baseline

В классификации базовые версии практически совпадают по качеству. Улучшенная версия self-made также показывает рост F1, подтверждая корректность реализованной регуляризации и class weights.

В регрессии значения почти полностью совпадают со sklearn Ridge + PolynomialFeatures, что говорит о правильности реализации матричных формул и расчетов.

## 4.e. Вывод

Реализация всех моделей выполнена корректно. Улучшения в self-made вариантах работают так же, как и улучшения в sklearn, что демонстрирует глубокое понимание линейных моделей, регуляризации и оптимизации.

## 4.f - 4.i. Улучшение self-made моделей

В итоговые версии были интегрированы:

- полиномиальные признаки;
- L2 регуляризация;
- корректировка весов классов;
- улучшенный градиентный спуск.

Self-made модели уверенно повторяют улучшения sklearn, а в некоторых случаях даже показывают чуть более стабильные результаты.

## 4.j. Общий итог

Работа выполнена полностью: исследованы baseline и улучшенные модели, реализованы собственные алгоритмы, проведено сравнение во всех режимах. Улучшенные версии уверенно превосходят исходные модели как для классификации, так и для регрессии, а самописные алгоритмы подтверждают понимание принципов логистической и линейной регрессии.