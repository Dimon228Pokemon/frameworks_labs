 Лабораторная работа №3
# Исследование модели Decision Tree (классификация и регрессия)

# 2. Создание бейзлайна и оценка качества

## 2.a. Baseline модели sklearn

Для начала была построена простая отправная точка — базовые модели `DecisionTreeClassifier` и `DecisionTreeRegressor` с параметрами по умолчанию. Перед обучением данные были предобработаны:

- Числовые признаки стандартизированы через `StandardScaler`
- Категориальные признаки переведены в one-hot формат (`OneHotEncoder`)
- Данные разделены на train/test с фиксированным `random_state`

Таким образом обеспечивается корректное сравнение моделей и предотвращение утечки данных.

## 2.b. Качество baseline моделей

#### Классификация - Baseline DecisionTreeClassifier

| Метрика    | Значение |
|------------|----------|
| Accuracy   | 0.7187   |
| Precision  | 0.3721   |
| Recall     | 0.3956   |
| F1         | 0.3835   |
| ROC-AUC    | 0.6032   |

**Вывод:** базовое дерево показывает среднее качество. Оно делает много ложноположительных и ложноотрицательных ошибок, F1 низкий, а ROC-AUC показывает слабую разделимость классов. Вероятная причина - переобучение из-за отсутствия ограничений по глубине и размеру листьев.

#### Регрессия - Baseline DecisionTreeRegressor

| Метрика | Значение |
|---------|----------|
| MAE     | 127 194  |
| RMSE    | 184 217  |
| R²      | 0.8918   |

**Вывод:** дерево регрессии также показывает признаки переобучения. Ошибка RMSE значительно растёт, и хотя R² остаётся высоким, модель очевидно обладает недостаточной обобщающей способностью из-за огромной глубины.

# 3. Улучшение бейзлайна

## 3.a. Сформулированные гипотезы

Для улучшения моделей были сформулированы следующие гипотезы.

**Классификация**

Решающие деревья чувствительны к переобучению, поэтому предполагалось, что:
- Ограничение глубины уменьшит вариативность и повысит F1
- Увеличение минимального числа образцов в листе может улучшить генерализацию
- Использование критерия `entropy` может давать более информативные разбиения
- Balancing классов может улучшить Recall для дефолтной группы

**Регрессия**

Похоже, что baseline дерево слишком гибкое, поэтому предполагалось:
- Ограничение глубины уменьшит RMSE
- Критерий `squared_error` стабилизирует разбиения
- Увеличение `min_samples_split` позволит построить более гладкую модель
- Использование пайплайна с `OneHotEncoder` обеспечит корректную обработку категорий

## 3.b. Проверка гипотез

Для подбора параметров был использован `GridSearchCV`.

**Классификация - DecisionTreeClassifier**

Лучшие параметры:
```python
{
 'class_weight': 'balanced',
 'criterion': 'entropy',
 'max_depth': 5,
 'min_samples_leaf': 10
}
```

**Регрессия - DecisionTreeRegressorr**

Лучшие параметры:
```python
{
 'criterion': 'squared_error',
 'max_depth': 9,
 'min_samples_leaf': 1,
 'min_samples_split': 50,
 'splitter': 'best'
}
```
Эти параметры значительно ограничивают дерево, уменьшая переобучение и улучшая общую стабильность.

## 3.c. Улучшенный бейзлайн

**Классификация**

Используем:
```python
DecisionTreeClassifier(
    criterion="entropy",
    max_depth=5,
    min_samples_leaf=10,
    class_weight="balanced",
    random_state=42
)
```

**Регрессия**

```python
Pipeline([
    ("preprocess", preprocessor),
    ("reg", DecisionTreeRegressor(
        criterion="squared_error",
        max_depth=9,
        min_samples_leaf=1,
        min_samples_split=50,
        splitter="best",
        random_state=42
    ))
])
```

## 3.d - 3.e. Оценка улучшенных моделей

### Классификация - Improved DecisionTreeClassifier

| Метрика    | Baseline | Improved |
|------------|----------|----------|
| Accuracy   | 0.7187   | 0.7738   |
| Precision  | 0.3721   | 0.4900   |
| Recall     | 0.3956   | 0.5524   |
| F1         | 0.3835   | 0.5193   |
| ROC-AUC    | 0.6032   | 0.7581   |

**Вывод:** улучшения дали сильный прирост по всем метрикам. Особенно выросли F1 и ROC-AUC, то есть модель стала лучше находить дефолтных клиентов и лучше разделять классы.

### Регрессия - Improved DecisionTreeRegressor

| Метрика | Baseline | Improved |
|---------|----------|----------|
| MAE     | 127 194  | 127 323  |
| RMSE    | 184 217  | 164 365  |
| R²      | 0.8918   | 0.9139   |

**Вывод:** улучшенная версия значительно уменьшила RMSE и увеличила R². Модель стала менее переобученной, давая более адекватные прогнозы.

## 3.f. Сравнение результатов

В обоих задачах улучшенные модели превосходят baseline:

- **Классификация:** сильный рост F1, Recall и ROC-AUC
- **Регрессия:** заметное снижение RMSE и рост R²

Это подтверждает, что ограничение глубины и размера листьев критически важно для стабильности решающих деревьев.

---

# 4. Имплементация алгоритмов вручную

## 4.a. Реализация моделей

Были реализованы две самописные модели:

- `MyDecisionTreeClassifier` - дерево для классификации на основе критерия Gini
- `MyDecisionTreeRegressor` - дерево для регрессии на основе MSE

Обе модели реализуют:

- Рекурсивное построение дерева
- Поиск лучшего разбиения
- Предсказание через обход дерева

Также созданы улучшенные версии с ограничениями глубины и минимального размера листа.

## 4.b - 4.c. Оценка самописной модели

#### Классификация - самописная модель DecisionTree

**Baseline (ручное дерево)**

| Метрика    | Значение |
|------------|----------|
| Accuracy   | 0.8163   |
| Precision  | 0.6522   |
| Recall     | 0.3632   |
| F1         | 0.4666   |
| ROC-AUC    | 0.6541   |

**Improved самописная модель**

| Метрика    | Значение |
|------------|----------|
| Accuracy   | 0.7710   |
| Precision  | 0.4842   |
| Recall     | 0.5426   |
| F1         | 0.5117   |
| ROC-AUC    | 0.6892   |

**Вывод:** улучшенная версия повторяет тренд sklearn — Recall и F1 растут, дерево меньше переобучается.

#### Регрессия - самописная модель DecisionTree

| Модель     | MAE      | RMSE     | R²       |
|------------|----------|----------|----------|
| Baseline   | 133 603  | 170 247  | 0.9076   |
| Improved   | 127 671  | 166 860  | 0.9113   |

**Вывод:** улучшенное самописное дерево даёт заметно более стабильный результат, повторяя тенденции sklearn.

## 4.d. Сравнение самописной модели и sklearn baseline

- Самописная модель baseline дерева по качеству очень близок к sklearn baseline
- Самописная модель improved стабильно повторяет тенденции улучшений sklearn
- Небольшие различия связаны с отсутствием оптимизаций C-level

Это подтверждает корректность реализации.

## 4.e. Итоговые выводы

В ходе работы были изучены, улучшены и реализованы вручную модели решающих деревьев для классификации и регрессии. Модели sklearn и реализованные вручную демонстрируют схожие тенденции, что указывает на правильно реализованную логику обучения и разбиений.

Улучшенные версии превосходят baseline как в классификации, так и в регрессии, а ручные реализации приближаются по качеству к библиотечным, подтверждая корректность построенных алгоритмов.